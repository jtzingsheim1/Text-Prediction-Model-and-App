---
title: "Word Prediction Project Milestone Report"
author: "Justin Z"
date: "June 30, 2019"
output: html_document
---


```{r setup, include = FALSE}

library(tidyverse)
library(quanteda)
library(knitr)

DLAndUnzipData <- function(data.filename = "Coursera-SwiftKey.zip") {
  # Downloads and unzips the capstone dataset if needed, returns folder name
  #
  # Args:
  #   data.filename: An optional name for the zip file, to replace the default
  #
  # Returns:
  #   A chacacter value of the name of the folder containing the data

  # Check if the file already exists, download if it does not
  if (!file.exists(data.filename)) {
    print("Downloading Data File")
    url <- paste0("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/",
                  "Coursera-SwiftKey.zip")
    download.file(url, data.filename)
  }
  
  # Check if the file is already unzipped, unzip if needed
  data.folder <- "final"
  if (!file.exists(data.folder)) {
    print("Unzipping Data File")
    unzip(data.filename)
  }
  return(data.folder)  # Return directory of unzipped file contents as character
}

AssembleCorpus <- function(n.lines,
                           file.selection = c("blogs", "news", "twitter"),
                           sub.dir = c("en_US", "de_DE", "fi_FI", "ru_RU")) {
  # Reads in specified number of lines from the specified file, assembles corpus
  #
  # Args:
  #   n.lines: The number of lines to read in from the text with readLines()
  #   file: Select which file to read from, one of: blogs, news, or twitter
  #   sub.dir: The subdirectory to read in files from, "en_US" by default
  #
  # Returns:
  #   A corpus of the text from the selected file, one "text" per line
  
  # Check and set arguments
  file.selection <- match.arg(file.selection)
  sub.dir <- match.arg(sub.dir)

  # Download and unzip the data, store folder name and file path
  filename <- paste(sub.dir, file.selection, "txt", sep = ".")  # Build file name
  filepath <- file.path(DLAndUnzipData(), sub.dir, filename)  # Build file path
  file.corpus <- filepath %>%
    readLines(n = n.lines) %>%  # Read in text
    corpus()  # Convert to corpus
  
  # Set metadata for the corpus
  docnames(file.corpus) <- paste0(file.selection, 1:ndoc(file.corpus))
  file.corpus$metadata$source <- filename
  file.corpus$metadata$file.size <- file.info(filepath)$size
  file.corpus$metadata$rows.read <- ndoc(file.corpus)
  
  # Return the corpus
  return(file.corpus)
}

AssembleSummary <- function(corpus.object) {
  # Assembles a data frame from the metadata of a corpus
  #
  # Args:
  #   corpus.object: The corpus from which to extract the metadata
  #
  # Returns:
  #   A data frame of the metadata
  
  # Extract metadata from corpus and convert to data frame
  corpus.metadata <- corpus.object %>%
    metacorpus() %>%
    as.data.frame(stringsAsFactors = FALSE)
  
  # Return the metadata as a data frame
  return(corpus.metadata)
}

```


## Overview

This is the first milestone report for my word prediction project which is for
the capstone project of the Data Science Specialization from Johns Hopkins
University on Coursera. The ultimate goal of the project is to create an
application that uses a model to predict which word comes next based on a
sequence of input words. The instructions for this milestone report say to
create an HTML report which summarizes the text files that will eventually form
the basis of the predictive model. The report should include figures that show
interesting features identified during exploratory data analysis.

The data for this project were downloaded through the course website and
pre-processed. The data were then explored for interesting features that will
guide the remaining portions of the project. Additional information such as the
full instructions, raw data, and initial script can be found in the
[GitHub repo][1] for this project.

[1]: xxxxx "GitHub repo for project"


## Introduction

A core part of the broader project is to create a predictive text model, and an
effective model should have large and diverse sample dataset to train on. The
course provides a large dataset which was obtained with a web crawler, and the
sources include: blogs, news articles, and Twitter. Data were collected in four
languages, but this project will only focus on English data.

The large size of the dataset could present challenges to both the memory and
processing speed of the machine that works with the data or the predictive
model. The table below summarizes the three files that were provided in the
English language. 

```{r load.data, include = FALSE, cache = TRUE}

# Set how many lines to read in from the text files
chunk.size <- 1000

# Read in text data and assemble into 3 corpora
blogs.corp <- AssembleCorpus(n.lines = chunk.size, file.selection = "blogs")
news.corp <- AssembleCorpus(n.lines = chunk.size, file.selection = "news")
twitter.corp <- AssembleCorpus(n.lines = chunk.size, file.selection = "twitter")

# Tokenize and clean text
# The predictive model will not attempt to predict: numbers, punctuation,
# symbols, twitter handles, hyphens, or urls, so these are all removed
blogs.tkn <- tokens(blogs.corp, remove_numbers = T, remove_punct = T,
                    remove_symbols = T, remove_twitter = T, remove_hyphens = T,
                    remove_url = T)
news.tkn <- tokens(news.corp, remove_numbers = T, remove_punct = T,
                    remove_symbols = T, remove_twitter = T, remove_hyphens = T,
                    remove_url = T)
twitter.tkn <- tokens(twitter.corp, remove_numbers = T, remove_punct = T,
                    remove_symbols = T, remove_twitter = T, remove_hyphens = T,
                    remove_url = T)

# Count the number of words in each token object and add to the corpus metadata
blogs.corp$metadata$word.count <- sum(ntoken(blogs.tkn))
news.corp$metadata$word.count <- sum(ntoken(news.tkn))
twitter.corp$metadata$word.count <- sum(ntoken(twitter.tkn))

```

```{r print.table, echo = FALSE, cache = TRUE}

# Construct a table that summarizes the corpora
summary.table <- bind_rows(AssembleSummary(blogs.corp),
                           AssembleSummary(news.corp),
                           AssembleSummary(twitter.corp))

# Update formatting of the table
summary.table <- summary.table %>%
  select(-created) %>%
  mutate(file.size = round(file.size / (1024 ^ 2), 1))

# Print the output table
column.names <- c("File Name", "File Size [Mb]", "Number of Lines",
                  "Number of Words")
kable(summary.table, col.names = column.names)

```

The table shows that the combined file size is over 500 Mb, so care must be
taken as these data loaded and processed. The word count listed in the table is
after some light pre-processing which removed some elements like numbers, urls,
and twitter handles. Elements like these can be removed since the predictive
model will not attempt to predict these types of features.



















